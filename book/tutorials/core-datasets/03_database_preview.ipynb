{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5276c910-43d9-4b68-961a-f385b9b748ad",
   "metadata": {},
   "source": [
    "# SnowEx Database Preview\n",
    "(15 minutes)\n",
    "\n",
    "<img src='https://i.gifer.com/FH7W.gif' alt='fireworks' style='width:250px' />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e05b2-ba43-4365-9194-679ea246acb6",
   "metadata": {},
   "source": [
    "Learning Objectives:\n",
    "\n",
    "* First taste of the database!\n",
    "* Code snippets to extract and prep data.\n",
    "* Generate ideas for project pitches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3a30b8-2b65-4511-b6ab-f27263376d29",
   "metadata": {},
   "source": [
    "## The Basics\n",
    "### How are the data contained?\n",
    "<img src='content/03_database-tables.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc326f2c-5bc2-4b7d-ab82-2c6345a7f3a0",
   "metadata": {},
   "source": [
    "## Set Up Computing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0348404d-e9af-43a6-b03e-bdf245c4a58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import datetime\n",
    "import re\n",
    "import geopandas as gpd\n",
    "\n",
    "# some mapping widgets\n",
    "import ipyleaflet\n",
    "from ipyleaflet import Map, GeoData, Rectangle, basemaps, LayersControl, basemap_to_tiles, TileLayer, SplitMapControl, Polygon\n",
    "import ipywidgets\n",
    "\n",
    "# database imports\n",
    "from snowexsql.db import get_db\n",
    "from snowexsql.data import PointData, LayerData, ImageData, SiteData\n",
    "from snowexsql.conversions import query_to_geopandas, query_to_pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dd84cd-7a0a-45d3-81e6-33933fd286cd",
   "metadata": {},
   "source": [
    "**POINTER -->** Notice where I import the **four** primary database tables. Can anyone call out what code line does this from the code block above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4f0d3f-d54e-428f-953b-6e99b8a9b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database\n",
    "db_name = 'snow:hackweek@db.snowexdata.org/snowex'\n",
    "engine, session = get_db(db_name)\n",
    "\n",
    "print('snowexsql database successfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1149b3a2-a87e-4c4b-b8ba-d5598fc01bcb",
   "metadata": {},
   "source": [
    "### What's the first thing you might like to do?\n",
    "Find overlapping data for data analysis comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661627b7-b045-467b-9a07-c6123b78084e",
   "metadata": {},
   "source": [
    "## Grand Mesa IOP (2020)\n",
    "### Example 1: Let's find all the pits that overlap with an airborne sensor of interest!\n",
    "First, it would be helpful to know, which of the airborne sensors are part of the database, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47cd21-f379-43fa-ad11-64be6109cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the session using .observers() to generate a list\n",
    "qry = session.query(ImageData.observers)\n",
    "\n",
    "# Locate all that are distinct\n",
    "airborne_sensors_list = session.query(ImageData.observers).distinct().all()\n",
    "\n",
    "print('list of airborne sensors by \"surveyor\" name: \\n', airborne_sensors_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d02ca8-a2d6-4afb-a09b-ccfe73b3220c",
   "metadata": {},
   "source": [
    "### 1a). Unsure of the flight date, but know which sensor you'd like to overlap with, here's how: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eaf87-8c62-4a20-93c3-13ada37c3486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airborne sensor from list above\n",
    "sensor = 'UAVSAR team, JPL'\n",
    "\n",
    "# Form a query on the Images table that returns Raster collection dates\n",
    "qry = session.query(ImageData.date)\n",
    "\n",
    "# Filter for UAVSAR data\n",
    "qry = qry.filter(ImageData.observers == sensor)\n",
    "\n",
    "# Grab the unique dates\n",
    "qry = qry.distinct()\n",
    "\n",
    "# Execute the query \n",
    "dates = qry.all() \n",
    "\n",
    "# Clean up the dates \n",
    "dates = [d[0] for d in dates] \n",
    "dlist = [str(d) for d in dates]\n",
    "dlist = \", \".join(dlist)\n",
    "print('%s flight dates are: %s' %(sensor, dlist))\n",
    "\n",
    "# Find all the snow pits done on these days\n",
    "qry = session.query(SiteData.geom, SiteData.site_id, SiteData.date)\n",
    "qry = qry.filter(SiteData.date.in_(dates))\n",
    "\n",
    "# Return a geopandas df\n",
    "df = query_to_geopandas(qry, engine)\n",
    "\n",
    "# View the returned dataframe!\n",
    "print(df.head())\n",
    "print(f'{len(df.index)} records returned!')\n",
    "\n",
    "# Close your session to avoid hanging transactions\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf18e03c-2ee7-4798-b805-83761b20f66b",
   "metadata": {},
   "source": [
    "### 1b).Want to select an exact flight date match? Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b861a-fed4-48c0-b463-193d52787afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a day from the list of dates\n",
    "dt = dates[0] \n",
    "\n",
    "# Find all the snow pits done on these days \n",
    "qry = session.query(SiteData.geom, SiteData.site_id, SiteData.date)\n",
    "qry = qry.filter(SiteData.date == dt)\n",
    "\n",
    "# Return a geopandas df\n",
    "df_exact = query_to_geopandas(qry, engine)\n",
    "\n",
    "print('%s pits overlap with %s on %s' %(len(df_exact), sensor, dt))\n",
    "\n",
    "# View snows pits that align with first UAVSAR date\n",
    "df_exact.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357fbd6-efd5-4ea2-bea5-ff62337a3b52",
   "metadata": {},
   "source": [
    "### 1c). Want to select a range of dates near the flight date? Here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db746402-510e-4a0d-a10f-cad953803eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Form a date range to query on either side of our chosen day \n",
    "date_range = [dt + i * datetime.timedelta(days=1) for i in [-1, 0, 1]]\n",
    "\n",
    "# Find all the snow pits done on these days \n",
    "qry = session.query(SiteData.geom, SiteData.site_id, SiteData.date)\n",
    "qry = qry.filter(SiteData.date.in_(date_range))\n",
    "\n",
    "# Return a geopandas df\n",
    "df_range = query_to_geopandas(qry, engine)\n",
    "\n",
    "# Clean up dates (for print statement only)\n",
    "dlist = [str(d) for d in date_range]\n",
    "dlist = \", \".join(dlist)\n",
    "\n",
    "print('%s pits overlap with %s on %s' %(len(df_range), sensor, dlist))\n",
    "\n",
    "# View snow pits that are +/- 1 day of the first UAVSAR flight date\n",
    "df_range.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73bc94cb-7789-4a21-bb08-fedc10c8cabf",
   "metadata": {},
   "source": [
    "### 1d). Have a known date that you wish to select data for, here's how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2163595b-12a2-4ae9-a36c-ce544d9857ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all the data that was collected on 2-12-2020\n",
    "dt = datetime.date(2020, 2, 12)\n",
    "\n",
    "#--------------- Point Data -----------------------------------\n",
    "# Grab all Point data instruments from our date\n",
    "point_instruments = session.query(PointData.instrument).filter(PointData.date == dt).distinct().all()\n",
    "point_type = session.query(PointData.type).filter(PointData.date == dt).distinct().all()\n",
    "\n",
    "# Clean up point data (i.e. remove tuple)\n",
    "point_instruments = [p[0] for p in point_instruments]\n",
    "point_instruments = \", \".join(point_instruments)\n",
    "point_type = [p[0] for p in point_type]\n",
    "point_type = \", \".join(point_type)\n",
    "print('Point data on %s are: %s, with the following list of parameters: %s' %(str(dt), point_instruments, point_type))\n",
    "\n",
    "#--------------- Layer Data -----------------------------------\n",
    "# Grab all Layer data instruments from our date\n",
    "layer_instruments = session.query(LayerData.instrument).filter(LayerData.date == dt).distinct().all()\n",
    "layer_type = session.query(LayerData.type).filter(LayerData.date == dt).distinct().all()\n",
    "\n",
    "# Clean up layer data \n",
    "layer_instruments = [l[0] for l in layer_instruments if l[0] is not None]\n",
    "layer_instruments = \", \".join(layer_instruments)\n",
    "layer_type = [l[0] for l in layer_type]\n",
    "layer_type = \", \".join(layer_type)\n",
    "print('\\nLayer Data on %s are: %s, with the following list of parameters: %s' %(str(dt), layer_instruments, layer_type))\n",
    "\n",
    "#--------------- Image Data -----------------------------------\n",
    "# Grab all Image data instruments from our date\n",
    "image_instruments = session.query(ImageData.instrument).filter(ImageData.date == dt).distinct().all()\n",
    "image_type = session.query(ImageData.type).filter(ImageData.date == dt).distinct().all()\n",
    "\n",
    "# Clean up image data (i.e. remove tuple)\n",
    "image_instruments = [i[0] for i in image_instruments]\n",
    "image_instruments = \", \".join(image_instruments)\n",
    "image_type = [i[0] for i in image_type]\n",
    "image_type = \", \".join(image_type)\n",
    "print('\\nImage Data on %s are: %s, with the following list of parameters: %s' %(str(dt), image_instruments, image_type))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f32a5cf-bcda-4305-ac07-25f3edf0ba7b",
   "metadata": {},
   "source": [
    "## Time Series (2020).... Almost! Using Grand Mesa IOP as a proxy, but code will transfer when Time Series data are available\n",
    "### Example 2: Let's plot some snow pit temperature profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e83cfcb-86c3-43d7-b756-f01857475af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# database imports (again)\n",
    "from snowexsql.db import get_db\n",
    "from snowexsql.data import PointData, LayerData, ImageData, SiteData\n",
    "from snowexsql.conversions import query_to_geopandas, query_to_pandas\n",
    "\n",
    "# add some database imports\n",
    "from sqlalchemy.sql import func\n",
    "import geoalchemy2.functions as gfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e36465-1e9a-4141-b29c-ba4bd3613fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database\n",
    "db_name = 'snow:hackweek@db.snowexdata.org/snowex'\n",
    "engine, session = get_db(db_name)\n",
    "\n",
    "print('snowexsql database successfully loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5519d9-2904-42ae-92ec-ced09b75f93e",
   "metadata": {},
   "source": [
    "### Do any sites have overlapping visits?\n",
    "Our best method to mimic the Time Series sampling design; snow pits have weekly visits at a single site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d1ba66-9c93-44bf-abc5-3d40eb76233c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the SiteData table to get list of site_id \n",
    "qry = session.query(SiteData.site_id)\n",
    "\n",
    "# execute the query\n",
    "results = qry.all()\n",
    "\n",
    "# find duplicates \n",
    "unqlist = []\n",
    "duplist = []\n",
    "\n",
    "for i in results:\n",
    "    if i not in unqlist:\n",
    "        unqlist.append(i)\n",
    "    else:\n",
    "        duplist.append(i)\n",
    "print('These {} sites had repeat visits during the 2020 Grand Mesa IOP campaign: {}'.format(len(duplist), duplist))\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a5b410-03ca-45e6-8e76-843a50ea4c86",
   "metadata": {},
   "source": [
    "### Use output to plot temperature profiles through time\n",
    "To do this we are going to use a nested for loop.\n",
    "- first loop over a sites (e.g. 1C1) and data parameter (e.g. temperature) in one query command\n",
    "- second loop over the dates and add a temperature profile plot to the figure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db76823f-3aea-4670-8a91-e64848c531a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up example\n",
    "sites = ['1C1', '5N10']\n",
    "\n",
    "# Setup the subplot for each site for each date\n",
    "fig, axes = plt.subplots(1, len(sites), figsize=(4*len(sites), 8))\n",
    "\n",
    "colors = ['k', 'm']\n",
    "\n",
    "# loop over sites and select data parameter\n",
    "for i, site in enumerate(sites):\n",
    "    # query the database by site name and for temperature profile data \n",
    "    qry = session.query(LayerData).filter(LayerData.site_id.in_([site])).filter(LayerData.type == 'temperature') # Grand Mesa, 2020 (repeated visits to mimic the Time Series when it becomes available)\n",
    "    \n",
    "    # convert to pandas dataframe  \n",
    "    df = query_to_pandas(qry, engine)  \n",
    "    \n",
    "    # create list of the unique dates (LayerData will have a lot of repeated dates, we only need a list per visit, not per measurement)\n",
    "    dates = sorted(df['date'].unique())\n",
    "    \n",
    "    # grab the plot for this site\n",
    "    ax = axes[i]\n",
    "\n",
    "    # counter to help with plotting\n",
    "    k=0\n",
    "    \n",
    "    \n",
    "    # loop over dates & plot temperature profile\n",
    "    for i, date in enumerate(dates):\n",
    "        \n",
    "        # grab the temperature profile \n",
    "        profile = df[df.date == date]\n",
    "        \n",
    "        # don't plot it unless there is data\n",
    "        if len(profile.index) > 0:            \n",
    "            \n",
    "            # sort by depth so samples that are taken out of order won't mess up the plot\n",
    "            profile = profile.sort_values(by='depth')\n",
    "\n",
    "            # cast as a float; layer profiles are always stored as strings\n",
    "            profile['value'] = profile['value'].astype(float)\n",
    "\n",
    "            # plot the temperature profile\n",
    "            ax.plot(profile['value'], \n",
    "                    profile['depth'], \n",
    "                    marker='o', \n",
    "                    color=colors[k],\n",
    "                    label=date)\n",
    "            ax.legend()\n",
    "            \n",
    "            k+=1 \n",
    "            \n",
    "# style all of the axes\n",
    "j=0  # counter for site titles \n",
    "for ax in axes.flatten():\n",
    "    ax.set_xlim(-12, 0)\n",
    "    ax.set_ylim(-10, 110)\n",
    "    ax.set_title(sites[j])\n",
    "    ax.set_xlabel('temperature [C]')\n",
    "    ax.set_ylabel('Depth [cm]')\n",
    "    j+=1\n",
    "\n",
    "session.close()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e79d97-3d91-4da1-8ba2-70d41da23e03",
   "metadata": {},
   "source": [
    "### So wait, how would I apply that to the Time Series Campaign data? \n",
    "uncomment the lines below once the Time Series data are integrated with the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79810aad-5d59-438d-a41c-cf5515fecf90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qry = session.query(LayerData.site_id).distinct() # notice before I used SiteDate, catch me later and I'll explain!\n",
    "# results = qry.all()\n",
    "# print(results)\n",
    "\n",
    "# # select the sites you want to use from the results list\n",
    "# qry = session.query(LayerData).filter(LayerData.site_id.in_(['Banner Open','Banner Snotel'])).filter(LayerData.type == 'temperature') # Time Series, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655cbfa8-ff91-4913-9612-e74c23b1fa4e",
   "metadata": {},
   "source": [
    "## Explore the Spatial Extent of Field Campaign Data\n",
    "### Example 3. Geographically, where do we have measurements? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8653696-e86b-4d49-b390-d310de2966d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox = [-125, 49, -102, 31]\n",
    "west, north, east, south = bbox\n",
    "bbox_ctr = [0.5*(north+south), 0.5*(west+east)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6035d1-daa0-43bb-82fa-6a1a285e79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Map(basemap=basemaps.CartoDB.Positron, center=bbox_ctr, zoom=4)\n",
    "rectangle = Rectangle(bounds=((south, west), (north, east))) #SW and NE corners of the rectangle (lat, lon)\n",
    "m.add_layer(rectangle)\n",
    "m\n",
    "\n",
    "# more info on available basemaps here: https://ipyleaflet.readthedocs.io/en/latest/map_and_basemaps/basemaps.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9131fa15-aa94-4226-a20b-6d64a44699b1",
   "metadata": {},
   "source": [
    "### Query the Database to add Spatial Data to Our Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5e65b9-e4a8-4fb2-b9d3-54297a819a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # database imports\n",
    "# from snowexsql.db import get_db\n",
    "# from snowexsql.data import PointData, LayerData, ImageData, SiteData\n",
    "# from snowexsql.conversions import query_to_geopandas, query_to_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba402b2-a46a-4d83-b9c3-0ef9f179fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the database\n",
    "db_name = 'snow:hackweek@db.snowexdata.org/snowex'\n",
    "engine, session = get_db(db_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a487a36-1afa-4f46-bc4f-7e4af4304dc5",
   "metadata": {},
   "source": [
    "### Let's find out where we have liquid water content (LWC) data \n",
    "\n",
    "**Pointer -->**  LWC data is in the LayerData table, because data at a single location were measured as a profile on the pit wall face (i.e has a vertical dimension) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d99ac56-7368-41d2-80fb-83d1696bd3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query the LayerData for all LWC values\n",
    "# qry = session.query(LayerData).filter(LayerData.type == 'lwc_vol')\n",
    "\n",
    "# query the geometry property of PointData\n",
    "# qry = session.query(LayerData.geom).distinct()\n",
    "\n",
    "# query the LayerData for all LWC values\n",
    "qry = session.query(LayerData.longitude, LayerData.latitude).filter(LayerData.type == 'lwc_vol').distinct() # \n",
    "\n",
    "# # query the geometry property of PointData\n",
    "# qry = query(LayerData.geom).distinct()\n",
    "\n",
    "# # limit the number of entries\n",
    "# qry = qry.limit(2000)\n",
    "\n",
    "# convert query to geopandas df\n",
    "df = query_to_pandas(qry, engine)\n",
    "\n",
    "df = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\n",
    "print(type(df))\n",
    "\n",
    "# # convert query to geopandas df\n",
    "# df = query_to_geopandas(qry, engine)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "# how many did we retrieve?\n",
    "print(f'{len(df.index)} records returned!')\n",
    "\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc14ae0-8ce5-4606-8007-4f7b0fefafd5",
   "metadata": {},
   "source": [
    "### Let's add these points to our map!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f30a0-4814-4973-a49c-80b33ec52d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Map(basemap=basemaps.CartoDB.Positron, center=bbox_ctr, zoom=4)\n",
    "\n",
    "# geo_data = GeoData(geo_dataframe = df)\n",
    "\n",
    "geo_data = GeoData(geo_dataframe = df,\n",
    "    style={'color': 'black', 'radius':8, 'fillColor': '#3366cc', 'opacity':0.5, 'weight':1.9, 'dashArray':'2', 'fillOpacity':0.6},\n",
    "    hover_style={'fillColor': 'red' , 'fillOpacity': 0.2},\n",
    "    point_style={'radius': 5, 'color': 'red', 'fillOpacity': 0.8, 'fillColor': 'blue', 'weight': 3},\n",
    "    name = 'lwc obs.')\n",
    "\n",
    "m.add_layer(geo_data) \n",
    "m.add_control(LayersControl())\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c0f28-dfa4-48a2-b28a-483ff7169fc5",
   "metadata": {},
   "source": [
    "## grab geojson from gist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3af03d0-8080-4bca-80c8-896ed646a76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import requests\n",
    "url = \"https://gist.githubusercontent.com/meganmason/21e6de1f7487b3e7defbe60ddde07e0e/raw/06939d8fffc0539a6014ee0d7c0c1d58938d9e8d/pits_raw.geojson\"\n",
    "gdf = gpd.read_file(url)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5fabcc-60b3-4c1a-b09e-2b32a5af667f",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b39a3a-9eb4-44f3-93a2-0a71b208fbbb",
   "metadata": {},
   "source": [
    "## References "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
