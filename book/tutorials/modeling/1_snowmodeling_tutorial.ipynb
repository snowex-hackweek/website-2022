{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "indirect-nevada",
   "metadata": {},
   "source": [
    "# Snow Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-avatar",
   "metadata": {},
   "source": [
    "**Contributors:** *Melissa Wrzesien<sup>1,2</sup>, Brendan McAndrew<sup>1,3</sup>, Jian Li<sup>4,5</sup>, Caleb Spradlin<sup>4,6</sup>*\n",
    "\n",
    "*<sup>1</sup> Hydrological Sciences Lab, NASA Goddard Space Flight Center, <sup>2</sup> University of Maryland, <sup>3</sup> Science Systems and Applications, Inc., <sup>4</sup> InuTeq, LLC, <sup>5</sup> Computational and Information Sciences and Technology Office (CISTO), NASA Goddard Space Flight Center, <sup>6</sup> High Performance Computing, NASA Goddard Space Flight Center*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-addiction",
   "metadata": {},
   "source": [
    "```{admonition} Learning Objectives\n",
    "- Learn about the role of modeling in a satellite mission\n",
    "- Open, explore, and visualize gridded model output\n",
    "- Compare model output to raster- and point-based observation datasets\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-acting",
   "metadata": {},
   "source": [
    "The goal of a future snow satellite is to provide improved understanding of global snow mass, as compared to current estimates. However, *a single sensor likely won't be able to accurately observe all types of snow in all conditions for the entire globe*. Instead, we need to combine future snow observations with other tools - including currently available satellite data and modeling. \n",
    "\n",
    "Models can also help to extend snow observations to areas where observations are not available. Data may be missing due to orbit gaps or masked out in regions of higher uncertainty. Remote sensing observations and models can be combined through **data assimilation**, a process where observations are used to constrain model estimates.\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/DAflowchart_v4.png\" style=\"width: 50%; border-radius:10px\">\n",
    "</center>\n",
    "\n",
    "The figure above shows a conceptual example of how satellite observations with orbit gaps could be combined with a model to produce results with no missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1b5954-85f8-4fb9-b142-dc8f6514a15d",
   "metadata": {},
   "source": [
    "## What is LIS?\n",
    "\n",
    "Since models will likely play a role in processing observations from a snow satellite mission, it is important to become comfortable working with gridded data products. Today we will use sample model output from **NASA's Land Information System (LIS)**.\n",
    "\n",
    ">The Land Information System is a software framework for land surface modeling and data assimilation developed with the goal of integrating satellite and ground-based observational data products with advanced modeling techniques to produce estimates of land surface states and fluxes.\n",
    "\n",
    "**TL;DR** LIS = land surface models + data assimilation\n",
    "\n",
    "<center>\n",
    "    <img src=\"./images/LIS_schematic.png\" style=\"width: 75%; border-radius:10px\">\n",
    "</center>\n",
    "\n",
    "One key feature LIS provides is flexibility to meet user needs. LIS consists of a collection of plug-ins, or modules, that allow users to design simulations with their choice of land surface model, meteorological forcing, data assimilation, hydrological routing, land surface parameters, and more. The plug-in based design also provides extensibility, making it easier to add new functionality to the LIS framework.\n",
    "\n",
    "Current efforts to expand support for snow modeling include implementation of snow modules, such as [SnowModel](https://doi.org/10.1175/JHM548.1) and [Crocus](http://www.umr-cnrm.fr/spip.php?article265&lang=en), into the LIS framework. These models, when run at the scale of ~100 meters, will enable simulation of wind redistribution, snow grain size, and other important processes for more accurate snow modeling.\n",
    "\n",
    "Development of LIS is led by the [Hydrological Sciences Laboratory](https://earth.gsfc.nasa.gov/hydro) at [NASA's Goddard Space Flight Center](https://www.nasa.gov/goddard)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a8eec-d314-4180-97ad-165f89320a18",
   "metadata": {},
   "source": [
    "## Working with Modeled Output\n",
    "\n",
    "Here we will use sample model output from a **LIS-SnowModel** simulation over a **western Colorado domain**. Daily estimates of **SWE, snow depth, and snow density** are written to output. The SnowModel simulation has a **spatial resolution of 100 m**. We've provided four years of output, though here we'll mostly use output from water year 2020.\n",
    "\n",
    "Below, we'll walk through how to open and interact with the LIS-SnowModel output. Our main objectives are to demonstrate how to do the following with a gridded dataset like LIS-SnowModel:\n",
    "* Understand attributes of model data file and variables available within\n",
    "* Create a spatial map of model output\n",
    "* Create time series at a point and averaged across domain\n",
    "* Compare LIS-SnowModel to raster and point data\n",
    "* Create a visualization for quick evaluation of model estimates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-bernard",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interface to Amazon S3 filesystem\n",
    "import s3fs\n",
    "\n",
    "# interact with n-d arrays\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# interact with tabular data (incl. spatial)\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# interactive plots\n",
    "import holoviews as hv\n",
    "import geoviews as gv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "# set bokeh as the holoviews plotting backend\n",
    "hv.extension('bokeh')\n",
    "# set holoviews backend to Bokeh\n",
    "gv.extension('bokeh')\n",
    "\n",
    "# used to find nearest grid cell to a given location\n",
    "from scipy.spatial import distance\n",
    "\n",
    "import fsspec\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from geoviews import opts\n",
    "from geoviews import tile_sources as gvts\n",
    "from datashader.colors import viridis\n",
    "import datashader\n",
    "from holoviews.operation.datashader import datashade, shade, dynspread, spread, rasterize\n",
    "from holoviews.streams import Selection1D, Params\n",
    "import panel as pn\n",
    "import param as pm\n",
    "import warnings\n",
    "import holoviews.operation.datashader as hd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pn.extension()\n",
    "pn.param.ParamMethod.loading_indicator =True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improved-floating",
   "metadata": {},
   "source": [
    "## Load the LIS-SnowModel Output\n",
    "\n",
    "The `xarray` library makes working with labelled n-dimensional arrays easy and efficient. If you're familiar with the `pandas` library it should feel pretty familiar.\n",
    "\n",
    "Here we load the LIS-SnowModel output into an `xarray.Dataset` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create S3 filesystem object\n",
    "s3 = s3fs.S3FileSystem(anon=False)\n",
    "\n",
    "# define the name of our S3 bucket\n",
    "bucket_name = 'eis-dh-hydro/SNOWEX-HACKWEEK'\n",
    "\n",
    "# define path to store on S3\n",
    "lis_output_s3_path_time_chunk = f's3://{bucket_name}/2022/ZARR/SURFACEMODEL/LIS_HIST_rechunkedV4.d01.zarr'\n",
    "lis_output_s3_path = f's3://{bucket_name}/2022/ZARR/SURFACEMODEL/LIS_HIST_default_chunks.d01.zarr/'\n",
    "\n",
    "# create key-value mapper for S3 object (required to read data stored on S3)\n",
    "lis_output_mapper = s3.get_mapper(lis_output_s3_path)\n",
    "lis_output_mapper_tc = s3.get_mapper(lis_output_s3_path_time_chunk)\n",
    "\n",
    "# open the dataset\n",
    "lis_output_ds = xr.open_zarr(lis_output_mapper, consolidated=True)\n",
    "lis_output_ds_tc = xr.open_zarr(lis_output_mapper_tc, consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-press",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "\n",
    "Display an interactive widget for inspecting the dataset by running a cell containing the variable name. Expand the dropdown menus and click on the document and database icons to inspect the variables and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-muslim",
   "metadata": {},
   "source": [
    "### Accessing Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-morgan",
   "metadata": {},
   "source": [
    "Dataset attributes (metadata) are accessible via the `attrs` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-pacific",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.attrs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-bulletin",
   "metadata": {},
   "source": [
    "### Accessing Variables\n",
    "\n",
    "Variables can be accessed using either **dot notation** or **square bracket notation**. Here we will stick with square bracket notation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-karaoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "# square bracket notation\n",
    "lis_output_ds['SM_SnowDepth_inst']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-traffic",
   "metadata": {},
   "source": [
    "```{admonition} Watch out for large datasets!\n",
    "Note that the 4-ish years of model output (1694 daily time steps for a domain of size 6650 x 4800) has a size of over **200 gb**! As we'll see below, this is just for an area over western Colorado. If we're interested in modeling the western United States or CONUS or even global at high resolution, we need to be prepared to work with some large datasets.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-fundamentals",
   "metadata": {},
   "source": [
    "### Dimensions and Coordinate Variables\n",
    "\n",
    "The dimensions and coordinate variable fields put the \"*labelled*\" in \"labelled n-dimensional arrays\":\n",
    "\n",
    "* **Dimensions:** labels for each dimension in the dataset (e.g., `time`)\n",
    "* **Coordinates:** labels for indexing along dimensions (e.g., `'2020-01-01'`)\n",
    "\n",
    "We can use these labels to select, slice, and aggregate the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-attack",
   "metadata": {},
   "source": [
    "### Subsetting in Space or Time\n",
    "\n",
    "`xarray` provides two methods for selecting or subsetting along coordinate variables:\n",
    "\n",
    "* index selection: `ds.isel(time=0)`\n",
    "* value selection `ds.sel(time='2020-01-01')`\n",
    "\n",
    "For example, we can use value selection to select based on the the coorindates of a given dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(time='2020-01-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-firewall",
   "metadata": {},
   "source": [
    "The `.sel()` approach also allows the use of shortcuts in some cases. For example, here we select all timesteps in the month of January 2020. Note that length of the time dimension is now only 31."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stock-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds.sel(time='2020-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-microphone",
   "metadata": {},
   "source": [
    "#### Latitude and Longitude\n",
    "\n",
    "You may have noticed that latitude (`lat`) and longitude (`lon`) are not listed as dimensions. This dataset would be easier to work with if `lat` and `lon` were coordinate variables and dimensions. Here we define a helper function that reads the spatial information from the dataset attributes, generates arrays containing the `lat` and `lon` values, and appends them to the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "junior-shell",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_latlon_coords(dataset: xr.Dataset)->xr.Dataset:\n",
    "    \"\"\"Adds lat/lon as dimensions and coordinates to an xarray.Dataset object.\"\"\"\n",
    "    \n",
    "    # get attributes from dataset\n",
    "    attrs = dataset.attrs\n",
    "    \n",
    "    # get x, y resolutions\n",
    "    dx = .001 #round(float(attrs['DX']), 3)\n",
    "    dy = .001 #round(float(attrs['DY']), 3)\n",
    "       \n",
    "    # get grid cells in x, y dimensions\n",
    "    ew_len = len(dataset['east_west'])\n",
    "    ns_len = len(dataset['north_south'])\n",
    "    \n",
    "    # get lower-left lat and lon\n",
    "    ll_lat = round(float(attrs['SOUTH_WEST_CORNER_LAT']), 3)\n",
    "    ll_lon = round(float(attrs['SOUTH_WEST_CORNER_LON']), 3)\n",
    "    \n",
    "    # calculate upper-right lat and lon\n",
    "    ur_lat =  41.5122 #ll_lat + (dy * ns_len)\n",
    "    ur_lon = -103.9831 #ll_lon + (dx * ew_len)\n",
    "    \n",
    "    # define the new coordinates\n",
    "    coords = {\n",
    "        # create an arrays containing the lat/lon at each gridcell\n",
    "        'lat': np.linspace(ll_lat, ur_lat, ns_len, dtype=np.float32, endpoint=False).round(4),\n",
    "        'lon': np.linspace(ll_lon, ur_lon, ew_len, dtype=np.float32, endpoint=False).round(4)\n",
    "    }\n",
    "    \n",
    "    lon_attrs = dataset.lon.attrs\n",
    "    lat_attrs = dataset.lat.attrs\n",
    "    \n",
    "    # rename the original lat and lon variables\n",
    "    dataset = dataset.rename({'lon':'orig_lon', 'lat':'orig_lat'})\n",
    "    # rename the grid dimensions to lat and lon\n",
    "    dataset = dataset.rename({'north_south': 'lat', 'east_west': 'lon'})\n",
    "    # assign the coords above as coordinates\n",
    "    dataset = dataset.assign_coords(coords)\n",
    "    dataset.lon.attrs = lon_attrs\n",
    "    dataset.lat.attrs = lat_attrs\n",
    "    \n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utility-girlfriend",
   "metadata": {},
   "source": [
    "Now that the function is defined, let's use it to append `lat` and `lon` coordinates to the LIS output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-commissioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds = add_latlon_coords(lis_output_ds)\n",
    "lis_output_ds_tc = add_latlon_coords(lis_output_ds_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-moldova",
   "metadata": {},
   "source": [
    "Inspect the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "lis_output_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-boards",
   "metadata": {},
   "source": [
    "Now `lat` and `lon` are listed as coordinate variables and have replaced the `north_south` and `east_west` dimensions. This will make it easier to spatially subset the dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tough-importance",
   "metadata": {},
   "source": [
    "#### Basic Spatial Subsetting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-reserve",
   "metadata": {},
   "source": [
    "We can use the `slice()` function on the `lat` and `lon` dimensions to select data between a range of latitudes and longitudes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment line below to work with the full domain -> this will be much slower!\n",
    "#lis_output_ds.sel(lat=slice(35.5, 41), lon=slice(-109, -104))\n",
    "\n",
    "# just Grand Mesa -> smaller domain for faster run times\n",
    "# note the smaller lat/lon extents in the dimensions\n",
    "lis_output_ds.sel(lat=slice(38.6, 39.4), lon=slice(-108.6, -107.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-submission",
   "metadata": {},
   "source": [
    "Notice how the sizes of the `lat` and `lon` dimensions have decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-draft",
   "metadata": {},
   "source": [
    "#### Subset Across Multiple Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mental-grounds",
   "metadata": {},
   "source": [
    "Now we will combine the above examples for subsetting both spatially and temporally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elementary-mystery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a range of dates to select\n",
    "wy_2020_slice = slice('2019-10-01', '2020-09-30')\n",
    "\n",
    "# define lat/lon for Grand Mesa area\n",
    "lat_slice = slice(38.6, 39.4)\n",
    "lon_slice = slice(-108.6, -107.1)\n",
    "\n",
    "# select the snow depth and subset to wy_2020_slice\n",
    "snd_GM_wy2020_ds = lis_output_ds['SM_SnowDepth_inst'].sel(time=wy_2020_slice, lat=lat_slice, lon=lon_slice)\n",
    "snd_GM_wy2020_ds_tc = lis_output_ds_tc['SM_SnowDepth_inst'].sel(time=wy_2020_slice, lat=lat_slice, lon=lon_slice)\n",
    "\n",
    "# inspect resulting dataset\n",
    "snd_GM_wy2020_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sublime-tanzania",
   "metadata": {},
   "source": [
    "```{admonition} Subset for more manageable sizes!\n",
    "We've now subsetted both spatially (down to just Grand Mesa domain) and temporally (water year 2020). Note the smaller size of the data array -> **a decrease from over 200 gb to 1.7 gb**! This smaller dataset will be much easier to work with, but feel free to try some of the commands with the full dataset, just give it a few minutes to process!\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-collect",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "We've imported two plotting libraries:\n",
    "\n",
    "* `matplotlib`: static plots\n",
    "* `hvplot`: interactive plots\n",
    "\n",
    "We can make a quick `matplotlib`-based plot for the subsetted data using the `.plot()` function supplied by `xarray.Dataset` objects. For this example, we'll select one day and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mineral-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple matplotlilb plot\n",
    "snd_GM_wy2020_ds.sel(time='2020-01-01').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "official-commonwealth",
   "metadata": {},
   "source": [
    "Similarly we can make an interactive plot using the `hvplot` accessor and specifying a `quadmesh` plot type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-toilet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hvplot based map\n",
    "snd_GM_20200101_plot = snd_GM_wy2020_ds.sel(time='2020-01-01').hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                                                                               xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                                                                               tiles='EsriImagery')\n",
    "\n",
    "snd_GM_20200101_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-american",
   "metadata": {},
   "source": [
    "Pan, zoom, and scroll around the map. Hover over the LIS-SnowModel data to see the data values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "anticipated-major",
   "metadata": {},
   "source": [
    "If we try to plot more than one time-step `hvplot` will also provide a time-slider we can use to scrub back and forth in time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-office",
   "metadata": {},
   "outputs": [],
   "source": [
    "snd_GM_wy2020_ds.sel(time='2020-01').hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                             xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                             tiles='EsriImagery')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-obligation",
   "metadata": {},
   "source": [
    "From here on out we will stick with `hvplot` for plotting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-worth",
   "metadata": {},
   "source": [
    "#### Timeseries Plots\n",
    "\n",
    "We can generate a timeseries for a given grid cell by selecting and calling the plot function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "digital-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define point to take timeseries (note: must be present in coordinates of dataset)\n",
    "ts_lon, ts_lat = (-107.8702, 39.0504)\n",
    "\n",
    "# plot timeseries\n",
    "snd_GM_wy2020_ds_tc.sel(lat=ts_lat, lon=ts_lon).hvplot.line(title=f'Snow Depth Timeseries @ Lon: {ts_lon}, Lat: {ts_lat}',\n",
    "                                                   xlabel='Date', ylabel='Snow Depth (m)') + \\\n",
    "    snd_GM_20200101_plot * gv.Points([(ts_lon, ts_lat)]).opts(size=10, color='red')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-muscle",
   "metadata": {},
   "source": [
    "In the next section we'll learn how to create a timeseries over a broader area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-vietnam",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "We can perform aggregation operations on the dataset such as `min()`, `max()`, `mean()`, and `sum()` by specifying the dimensions along which to perform the calculation.\n",
    "\n",
    "For example we can calculate the daily mean snow depth for the region around Grand Mesa for water year 2020:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-chart",
   "metadata": {},
   "source": [
    "### Area Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take area-averaged mean at each timestep\n",
    "mean_snd_GM_wy2020_ds = snd_GM_wy2020_ds_tc.mean(['lat', 'lon'])\n",
    "\n",
    "# inspect the dataset\n",
    "mean_snd_GM_wy2020_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aerial-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot timeseries (hvplot knows how to plot based on dataset's dimensionality!)\n",
    "mean_snd_GM_wy2020_ds.hvplot(title='Mean LIS-SnowModel Snow Depth for Grand Mesa area', xlabel='Date', ylabel='Snow Depth (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-shannon",
   "metadata": {},
   "source": [
    "## Comparing Model Output\n",
    "\n",
    "Now that we're familiar with the model output, let's compare it to two other datasets: SNODAS (raster) and SNOTEL (point)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-class",
   "metadata": {},
   "source": [
    "### LIS-SnowModel (raster) vs. SNODAS (raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-clear",
   "metadata": {},
   "source": [
    "First, we'll load the SNODAS dataset which we also have hosted on S3 as a Zarr store. This command will take a minute or two to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNODAS dataset\n",
    "\n",
    "#snodas depth\n",
    "key = \"SNODAS/snodas_snowdepth_20161001_20200930.zarr\"    \n",
    "snodas_depth_ds = xr.open_zarr(s3.get_mapper(f\"{bucket_name}/{key}\"), consolidated=True)\n",
    "\n",
    "# apply scale factor to convert to meters (0.001 per SNODAS user guide)\n",
    "snodas_depth_ds = snodas_depth_ds * 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-legislature",
   "metadata": {},
   "source": [
    "Next we define a helper function to extract the (lon, lat) of the nearest grid cell to a given point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-dover",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_grid(ds, pt):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the nearest lon and lat to pt in a given Dataset (ds).\n",
    "    \n",
    "    pt : input point, tuple (longitude, latitude)\n",
    "    output:\n",
    "        lon, lat\n",
    "    \"\"\"\n",
    "    \n",
    "    if all(coord in list(ds.coords) for coord in ['lat', 'lon']):\n",
    "        df_loc = ds[['lon', 'lat']].to_dataframe().reset_index()\n",
    "    else:\n",
    "        df_loc = ds[['orig_lon', 'orig_lat']].isel(time=0).to_dataframe().reset_index()\n",
    "    \n",
    "    loc_valid = df_loc.dropna()\n",
    "    pts = loc_valid[['lon', 'lat']].to_numpy()\n",
    "    idx = distance.cdist([pt], pts).argmin()\n",
    "    \n",
    "    return loc_valid['lon'].iloc[idx], loc_valid['lat'].iloc[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunset-dynamics",
   "metadata": {},
   "source": [
    "The next cell will look pretty similar to what we did earlier to plot a timeseries of a single point in the LIS-SnowModel data. The general steps are:\n",
    "\n",
    "* Extract the coordinates of the SNODAS grid cell nearest to our LIS-SnowModel grid cell (`ts_lon` and `ts_lat` from earlier)\n",
    "* Subset the SNODAS and LIS-SnowModel data to the grid cells and date ranges of interest\n",
    "* Create the plots!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-relative",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop off irrelevant variables\n",
    "drop_vars = ['orig_lat', 'orig_lon']\n",
    "lis_output_ds = lis_output_ds.drop(drop_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-globe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lon, lat of snodas grid cell nearest to the LIS coordinates we used earlier\n",
    "snodas_ts_lon, snodas_ts_lat = nearest_grid(snodas_depth_ds, (ts_lon, ts_lat))\n",
    "\n",
    "# define a date range to plot (shorter = quicker for demo)\n",
    "start_date, end_date = ('2020-01-01', '2020-06-01')\n",
    "plot_daterange = slice(start_date, end_date)\n",
    "\n",
    "# select SNODAS grid cell and subset to plot_daterange\n",
    "snodas_snd_subset_ds = snodas_depth_ds.sel(lon=snodas_ts_lon,\n",
    "                                             lat=snodas_ts_lat,\n",
    "                                             time=plot_daterange)\n",
    "\n",
    "# select LIS grid cell and subset to plot_daterange\n",
    "lis_snd_subset_ds = lis_output_ds['SM_SnowDepth_inst'].sel(lat=ts_lat,\n",
    "                                                        lon=ts_lon,\n",
    "                                                        time=plot_daterange)\n",
    "\n",
    "# create SNODAS snow depth plot\n",
    "snodas_snd_plot = snodas_snd_subset_ds.hvplot(label='SNODAS')\n",
    "\n",
    "# create LIS snow depth plot\n",
    "lis_snd_plot = lis_snd_subset_ds.hvplot(label='LIS-SnowModel')\n",
    "\n",
    "# create SNODAS vs LIS snow depth plot\n",
    "lis_vs_snodas_snd_plot = (lis_snd_plot * snodas_snd_plot)\n",
    "\n",
    "# display the plot\n",
    "lis_vs_snodas_snd_plot.opts(title=f'Snow Depth @ Lon: {ts_lon}, Lat: {ts_lat}',\n",
    "                            legend_position='right',\n",
    "                            xlabel='Date',\n",
    "                            ylabel='Snow Depth (m)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-plastic",
   "metadata": {},
   "source": [
    "### LIS-SnowModel (raster) vs. SNODAS (raster) vs. SNOTEL (point)\n",
    "\n",
    "Now let's add SNOTEL point data to our plot.\n",
    "\n",
    "First, we're going to define some helper functions to load the SNOTEL data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv containing metadata for SNOTEL sites in a given state (e.g,. 'colorado')\n",
    "def load_site(state):\n",
    "    \n",
    "    # define the path to the file\n",
    "    key = f\"SNOTEL/snotel_{state}.csv\"\n",
    "    \n",
    "    # load the csv into a pandas DataFrame\n",
    "    df = pd.read_csv(s3.open(f's3://{bucket_name}/{key}', mode='r'))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# load SNOTEL data for a specific site\n",
    "def load_snotel_txt(state, var):\n",
    "    \n",
    "    # define the path to the file\n",
    "    key = f\"SNOTEL/snotel_{state}{var}_20162020.txt\"\n",
    "    \n",
    "    # determine how many lines to skip in the file (they start with #)\n",
    "    fh = s3.open(f\"{bucket_name}/{key}\")\n",
    "    lines = fh.readlines()\n",
    "    skips = sum(1 for ln in lines if ln.decode('ascii').startswith('#'))\n",
    "    \n",
    "    # load the data into a pandas DataFrame\n",
    "    df = pd.read_csv(s3.open(f\"s3://{bucket_name}/{key}\"), skiprows=skips)\n",
    "    \n",
    "    # convert the Date column from strings to datetime objects\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organized-hepatitis",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial let's load the SNOTEL data for sites in Colorado. We'll pick one site to plot in a few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handmade-engagement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNOTEL snow depth for Colorado into a dictionary\n",
    "snotel_depth = {'CO': load_snotel_txt('CO', 'depth')}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitting-lunch",
   "metadata": {},
   "source": [
    "We'll need another helper function to load the depth data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-stock",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get snotel depth\n",
    "def get_depth(state, site, start_date, end_date):\n",
    "    \n",
    "    # grab the depth for the given state (e.g., CO)\n",
    "    df = snotel_depth[state]\n",
    "    \n",
    "    # define a date range mask\n",
    "    mask = (df['Date'] >= start_date) & (df['Date'] <= end_date)\n",
    "    \n",
    "    # use mask to subset between time range\n",
    "    df = df.loc[mask]\n",
    "    \n",
    "    # extract timeseries for the given site\n",
    "    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-blank",
   "metadata": {},
   "source": [
    "Load the site metadata for Colorado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_sites = load_site('colorado')\n",
    "\n",
    "# peek at the first 5 rows\n",
    "co_sites.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-citizen",
   "metadata": {},
   "source": [
    "The point we've been using so far in the tutorial actually corresponds to the coordinates for the Park Reservoir SNOTEL on Grand Mesa! Let's extract the site data for that point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the depth data by passing the site name to the get_depth() function\n",
    "park_res_snd_df = get_depth('CO', 'Park Reservoir (682)', start_date, end_date)\n",
    "\n",
    "# convert from cm to m\n",
    "park_res_snd_df = park_res_snd_df / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-packaging",
   "metadata": {},
   "source": [
    "Now we're ready to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SNOTEL plot\n",
    "park_res_plot = park_res_snd_df.hvplot(label='SNOTEL')\n",
    "\n",
    "# combine the SNOTEl plot with the LIS vs SNODAS plot\n",
    "(lis_vs_snodas_snd_plot * park_res_plot).opts(title=f'Snow Depth @ Lon: {ts_lon}, Lat: {ts_lat}', legend_position='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-europe",
   "metadata": {},
   "source": [
    "# Interactive Data Exploration\n",
    "\n",
    "We've now built up the separate pieces for visualizing LIS-SnowModel output spatially and temporally, and we've combined our time series with those from SNOTEL stations and SNODAS. Now we can bring it all together in an interactive tool.\n",
    "\n",
    "The code in the cells below will generate an interactive panel for comparing of LIS-SnowModel output, SNODAS, and SNOTEL snow depth and snow water equivalent at SNOTEL site locations. \n",
    "\n",
    "**Note: some cells below take several minutes to run and some aspects of the interactive widgets may not work in the rendered version on the Hackweek site.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-trinidad",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "elect-activity",
   "metadata": {},
   "source": [
    "### SNOTEL Sites info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-professional",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary linking state names and abbreviations\n",
    "snotel = {\"CO\" : \"colorado\",\n",
    "          \"ID\" : \"idaho\",\n",
    "          \"NM\" : \"newmexico\",\n",
    "          \"UT\" : \"utah\",\n",
    "          \"WY\" : \"wyoming\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNOTEL site metadata for sites in the given state\n",
    "def load_site(state):\n",
    "    \n",
    "    # define path to file\n",
    "    key = f\"SNOTEL/snotel_{state}.csv\"\n",
    "    \n",
    "    # load csv into pandas DataFrame\n",
    "    df = pd.read_csv(s3.open(f'{bucket_name}/{key}', mode='r'))\n",
    "    \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accomplished-rhythm",
   "metadata": {},
   "source": [
    "### SNOTEL Depth & SWE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-alaska",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_snotel_txt(state, var):\n",
    "    \n",
    "    # define path to file\n",
    "    key = f\"SNOTEL/snotel_{state}{var}_20162020.txt\"\n",
    "\n",
    "    # open text file\n",
    "    fh = s3.open(f\"{bucket_name}/{key}\")\n",
    "    \n",
    "    # read each line and note those that begin with '#'\n",
    "    lines = fh.readlines()\n",
    "    skips = sum(1 for ln in lines if ln.decode('ascii').startswith('#'))\n",
    "    \n",
    "    # load txt file into pandas DataFrame (skipping lines beginning with '#')\n",
    "    df = pd.read_csv(s3.open(f\"{bucket_name}/{key}\"), skiprows=skips)\n",
    "    \n",
    "    # convert Date column from str to pandas datetime objects\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-tomorrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load SNOTEL depth & swe into dictionaries\n",
    "\n",
    "# define empty dicts\n",
    "snotel_depth = {}\n",
    "snotel_swe = {}\n",
    "\n",
    "# loop over states and load SNOTEL data\n",
    "for state in snotel.keys():\n",
    "    print(f\"Loading state {state}\")\n",
    "    snotel_depth[state] = load_snotel_txt(state, 'depth')\n",
    "    snotel_swe[state] = load_snotel_txt(state, 'swe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-probability",
   "metadata": {},
   "source": [
    "### SNODAS SWE\n",
    "\n",
    "We've already loaded in SNODAS snow depths, but here we'll load in SWE. As above, this cell might take a few minutes to run since it's a large dataset to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-phoenix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load snodas swe data\n",
    "key = \"SNODAS/snodas_swe_20161001_20200930.zarr\"\n",
    "snodas_swe_ds = xr.open_zarr(s3.get_mapper(f\"{bucket_name}/{key}\"), consolidated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-behalf",
   "metadata": {},
   "source": [
    "### LIS-SnowModel Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-college",
   "metadata": {},
   "source": [
    "We've already read in the LIS-SnowModel data above. Here we subset by time for October 2019 - June 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hairy-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset LIS data for winter 2020 \n",
    "time_range = slice('2019-10-01', '2020-06-30')\n",
    "lis_sf = lis_output_ds_tc.sel(time=time_range)\n",
    "lis_sf = lis_sf.drop(drop_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-section",
   "metadata": {},
   "source": [
    "In the next cell, we extract the data variable names and timesteps from the LIS outputs. These will be used to define the widget options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather metadata from LIS\n",
    "# get variable names:string\n",
    "vnames = list(lis_sf.data_vars)\n",
    "print(vnames)\n",
    "\n",
    "# get time-stamps:string\n",
    "tstamps = list(np.datetime_as_string(lis_sf.time.values, 'D'))\n",
    "print(len(tstamps), tstamps[0], tstamps[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-portal",
   "metadata": {},
   "source": [
    "By default, the `holoviews` plotting library automatically adjusts the range of plot colorbars based on the range of values in the data being plotted. This may not be ideal when comparing data on different timesteps. In the next cell we assign the upper and lower bounds for each data variable which we'll later use to set a static colorbar range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chubby-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_lims = {'SM_SWE_inst': (0.0, 3.0),\n",
    " 'SM_SnowDensity_inst': (100, 550.0),\n",
    " 'SM_SnowDepth_inst': (0.0, 6.5)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-folder",
   "metadata": {},
   "source": [
    "## Interactive Widget\n",
    "\n",
    "### SNOTEL Site Map and Timeseries\n",
    "\n",
    "The two cells that follow will create an interactive panel for comparing LIS-SnowModel, SNODAS, and SNOTEL snow depth and snow water equivalent. The SNOTEL site locations are plotted as points on an interactive map for each state. Hover over the sites to view metadata and click on a site to generate a timeseries!\n",
    "\n",
    "**Note: it will take some time for the timeseries to display.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-liechtenstein",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get snotel depth\n",
    "def get_depth(state, site, ts, te):\n",
    "    df = snotel_depth[state]\n",
    "    \n",
    "    # subset between time range\n",
    "    mask = (df['Date'] >= ts) & (df['Date'] <= te)\n",
    "    df = df.loc[mask]\n",
    "    \n",
    "    # extract timeseries for the site\n",
    "    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')\n",
    "\n",
    "# get snotel swe\n",
    "def get_swe(state, site, ts, te):\n",
    "    df = snotel_swe[state]\n",
    "    \n",
    "    # subset between time range\n",
    "    mask = (df['Date'] >= ts) & (df['Date'] <= te)\n",
    "    df = df.loc[mask]\n",
    "    \n",
    "    # extract timeseries for the site\n",
    "    return pd.concat([df.Date, df.filter(like=site)], axis=1).set_index('Date')\n",
    "\n",
    "# co-locate site & LIS-SnowModel model cell\n",
    "def nearest_grid(pt):\n",
    "    # pt : input point, tuple (longtitude, latitude)\n",
    "    # output:\n",
    "    #        x_idx, y_idx \n",
    "    loc_valid = df_loc.dropna()\n",
    "    pts = loc_valid[['lon', 'lat']].to_numpy()\n",
    "    idx = distance.cdist([pt], pts).argmin()\n",
    "\n",
    "    return loc_valid['east_west'].iloc[idx], loc_valid['north_south'].iloc[idx]\n",
    "\n",
    "# get LIS-SnowModel variable \n",
    "def var_subset(dset, v, lon, lat, ts, te):\n",
    "    return dset[v].sel(lon=lon, lat=lat, method=\"nearest\").sel(time=slice(ts, te)).load()\n",
    "\n",
    "# line plots\n",
    "def line_callback(index, state, vname, ts_tag, te_tag):\n",
    "    sites = load_site(snotel[state])\n",
    "    row = sites.iloc[1]\n",
    "    \n",
    "    tmp = var_subset(lis_sf, vname, row.lon, row.lat, ts_tag, te_tag) \n",
    "    xr_sf = xr.zeros_like(tmp)\n",
    "    \n",
    "    xr_snodas = xr_sf\n",
    "    \n",
    "    ck = get_depth(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})\n",
    "    xr_snotel = xr.zeros_like(ck)\n",
    "    \n",
    "    if not index:\n",
    "        title='Var: -- Lon: -- Lat: --'\n",
    "        return (xr_sf.hvplot(title=title, color='blue', label='LIS-SnowModel') \\\n",
    "                * xr_snotel.hvplot(color='red', label='SNOTEL') \\\n",
    "                * xr_snodas.hvplot(color='green', label='SNODAS')).opts(legend_position='right')\n",
    "        \n",
    "\n",
    "    else:\n",
    "        sites = load_site(snotel[state])\n",
    "        first_index = index[0]\n",
    "        row = sites.iloc[first_index]\n",
    "    \n",
    "        \n",
    "        xr_sf = var_subset(lis_sf, vname, row.lon, row.lat, ts_tag, te_tag)\n",
    "    \n",
    "        vs = vname.split('_')[1]\n",
    "        title=f'Var: {vs} Lon: {row.lon} Lat: {row.lat}'\n",
    "\n",
    "        \n",
    "        # update snotel data \n",
    "        if 'depth' in vname.lower():\n",
    "            xr_snotel =  get_depth(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})*0.01\n",
    "            xr_snodas =  var_subset(snodas_depth_ds, 'SNOWDEPTH', row.lon, row.lat, ts_tag, te_tag)\n",
    "        \n",
    "        if 'swe' in vname.lower():\n",
    "            xr_snotel =  get_swe(state, row.site_name, ts_tag, te_tag).to_xarray().rename({'Date': 'time'})/1000\n",
    "            xr_snodas =  var_subset(snodas_swe_ds, 'SWE', row.lon, row.lat, ts_tag, te_tag)/1000\n",
    "\n",
    "    \n",
    "        return xr_sf.hvplot(title=title, color='blue', label='LIS-SnowModel') \\\n",
    "               * xr_snotel.hvplot(color='red', label='SNOTEL') \\\n",
    "               * xr_snodas.hvplot(color='green', label='SNODAS')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threatened-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# satic snowdepth as background\n",
    "dds = lis_sf['SM_SnowDepth_inst'].sel(time='2020-02-01').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assigned-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the panel that will display a static plot of snow depth. Snotel sites will be plotted on top\n",
    "img = dds.hvplot.quadmesh(geo=True, rasterize=True, project=True,\n",
    "                             xlabel='lon', ylabel='lat', cmap='viridis',\n",
    "                             clim=(0,1))\n",
    "snow_depth_bg=hd.regrid(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-baker",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sites on map\n",
    "def plot_points(state):  \n",
    "    # dataframe to hvplot obj Points\n",
    "    sites=load_site(snotel[state])\n",
    "    pts_opts=dict(size=10, nonselection_alpha=0.5,tools=['tap', 'hover'])\n",
    "    #site_points=sites.hvplot.points(x='lon', y='lat', c='elev', cmap='fire', geo=True, hover_cols=['site_name', 'ntwk', 'state', 'lon', 'lat']).opts(**pts_opts)  \n",
    "    site_points=sites.hvplot.points(x='lon', y='lat', color='black',geo=True, hover_cols=['site_name', 'ntwk', 'state', 'lon', 'lat']).opts(**pts_opts)  \n",
    "    return site_points\n",
    "\n",
    "# base map\n",
    "tiles = gvts.OSM()\n",
    "\n",
    "# state widget\n",
    "state_select = pn.widgets.Select(options=list(snotel.keys()), name=\"State\")\n",
    "state_stream = Params(state_select, ['value'], rename={'value':'state'})\n",
    "\n",
    "# variable widget\n",
    "var_select = pn.widgets.Select(options=['SM_SnowDepth_inst', 'SM_SWE_inst'], name=\"LIS Variable List\")\n",
    "var_stream = Params(var_select, ['value'], rename={'value':'vname'})\n",
    "\n",
    "# date range widget\n",
    "date_fmt = '%Y-%m-%d'\n",
    "sdate_input = pn.widgets.DatetimeInput(name='Start date', value=dt(2019,10,1),start=dt.strptime(tstamps[0], date_fmt), end=dt.strptime(tstamps[-1], date_fmt), format=date_fmt)\n",
    "sdate_stream = Params(sdate_input, ['value'], rename={'value':'ts_tag'})\n",
    "edate_input = pn.widgets.DatetimeInput(name='End date', value=dt(2020,6,30),start=dt.strptime(tstamps[0], date_fmt), end=dt.strptime(tstamps[-1], date_fmt),format=date_fmt)\n",
    "edate_stream = Params(edate_input, ['value'], rename={'value':'te_tag'})\n",
    "\n",
    "# generate site points as dynamic map\n",
    "# plots points and calls plot_points() when user selects a site\n",
    "site_dmap = hv.DynamicMap(plot_points, streams=[state_stream]).opts(height=400, width=600)\n",
    "# pick site\n",
    "select_stream = Selection1D(source=site_dmap)\n",
    "\n",
    "# link widgets to callback function\n",
    "line = hv.DynamicMap(line_callback, streams=[select_stream, state_stream, var_stream, sdate_stream, edate_stream])\n",
    "\n",
    "# create panel layout\n",
    "pn.Row(snow_depth_bg*site_dmap*tiles, pn.Column(state_select, var_select, pn.Row(sdate_input, edate_input), line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab72d40b-7507-4b77-a31f-efc606c658d0",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "You now know how to see what variables are available in a typical model output file and you've learned how to inspect gridded model estimates both spatially and temporally. Below are a few exercises for both practicing the above skills and for becoming more familiar with the sample model output.\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "So far we've mostly worked with the Grand Mesa region. Can you spatially subset the data to inspect the LIS-SnowModel estimates for the Front Range? What about for Senator Beck basin? Try to use hvplot to plot a map of SWE values for February 2020.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "Here we focused on water year 2020. Can you select a point (maybe the same Park Reservoir SNOTEL point we worked with here) and make a multi-year time series?\n",
    "\n",
    "## Exercise 3 (for an extra challenge!)\n",
    "\n",
    "We learned about the SnowEx database in a tutorial earlier this week. Can you create a new notebook for combining the model output with your choice of field observation available in the database? Does the model over or underestimate the SnowEx observation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d91e9eb-ae34-4b50-9817-bab86b95c66f",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "We're now more familiar with model output and how to interact with it in Python. The code in this notebook is a great jumping off point for developing more advanced comparisons and interactive widgets.\n",
    "\n",
    "The Python code can be adapted to other LIS simulations and to other model output as well, with minor modifications. Anyone interested in testing your new skills can combine what you learned here with the other SnowEx Hackweek tutorials - try comparing the LIS-SnowModel output with other snow observations collected during the 2017 field campaign!\n",
    "\n",
    "For more information on [NASA's Land Information System](https://lis.gsfc.nasa.gov/) please see the links below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-midwest",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "**Links**\n",
    "\n",
    "* [LIS Website](https://lis.gsfc.nasa.gov/)\n",
    "* [LIS on GitHub](https://github.com/NASA-LIS/LISF)\n",
    "* [LIS Documentation](https://github.com/NASA-LIS/LISF/blob/master/docs/)\n",
    "* [LIS Test Case Walkthrough](https://github.com/bmcandr/lis-public-tc-walkthrough)\n",
    "\n",
    "**References**\n",
    "\n",
    "* Kumar, S.V., C.D. Peters-Lidard, Y. Tian, P.R. Houser, J. Geiger, S. Olden, L. Lighty, J.L. Eastman, B. Doty, P. Dirmeyer, J. Adams, K. Mitchell, E. F. Wood, and J. Sheffield, 2006: Land Information System - An interoperable framework for high resolution land surface modeling. Environ. Modeling & Software, 21, 1402-1415, [doi:10.1016/j.envsoft.2005.07.004](https://doi.org/10.1016/j.envsoft.2005.07.004)\n",
    "\n",
    "* Peters-Lidard, C.D., P.R. Houser, Y. Tian, S.V. Kumar, J. Geiger, S. Olden, L. Lighty, B. Doty, P. Dirmeyer, J. Adams, K. Mitchell, E.F. Wood, and J. Sheffield, 2007: High-performance Earth system modeling with NASA/GSFC's Land Information System. Innovations in Systems and Software Engineering, 3(3), 157-165, [doi:10.1007/s11334-007-0028-x](https://doi.org/10.1007/s11334-007-0028-x)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
